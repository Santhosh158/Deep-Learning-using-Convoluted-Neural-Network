---
title: "Assignment3-Rajamanickam-Google Image Classification"
author: "Santhosh Babu Rajamanickam Natarajan"
date: "15/04/2021"
output: html_document
---

## Introduction

**Google Images Data**  
Kaggle dataset URL: https://www.kaggle.com/duttadebadri/image-classification  


The dataset consists of scraped images from google which are split into four different categories as below  
* Architecture  
* Art and culture  
* Food and Drinks  
* Travel and culture  

- Here, we are building a model to classify the images into one of the categories - 'architecture' and 'art and culture'.  
- The images varies with color, diversity, structure, people and various other attributes. We could find more images with people in 'Art and culture' category compared to 'Architecture' which consists of buildings and structures.  


### Load packages

Loading `keras`,`reticulate`, `tidyverse`, `iamger`,`colorfindr` packages. 

```{r load-packages, message=FALSE, warning=FALSE}
library(keras)
library(reticulate)
library(tidyverse)
library(imager)
library(colorfindr)
```

### Creating the dataset directories for test, train and validation

* The downloaded image dataset contains many image files for each of the four categories. The image files under the categories 'architecture' and 'art and culture' will be considered for analysis.  
* In the following code, the images dataset containing the architecture and art and culture images is split into test, train and validation directories.  
```{r Creating directories, message=FALSE, warning=FALSE}
 # we will only use the labelled data
base_dir <- "data/images_test" # to store a sebset of data that we are going to use
dir.create(base_dir)
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)
train_arc_dir <- file.path(train_dir, "architecture")
dir.create(train_arc_dir)
train_art_dir <- file.path(train_dir, "artculture")
dir.create(train_art_dir)
validation_arc_dir <- file.path(validation_dir, "architecture")
dir.create(validation_arc_dir)
validation_art_dir <- file.path(validation_dir, "artculture")
dir.create(validation_art_dir)
test_arc_dir <- file.path(test_dir, "architecture")
dir.create(test_arc_dir)
test_art_dir <- file.path(test_dir, "artculture")
dir.create(test_art_dir)
```



### Renaming image files
```{r Renaming images, message=FALSE, warning=FALSE, results='hide'}

invisible({capture.output({
arc_dir <- "C:/CIS8392/data/archive/images/images/architecure"
art_dir <- "C:/CIS8392/data/archive/images/images/art and culture"

architecture_images <- list.files("C:/CIS8392/data/archive/images/images/architecure", pattern = "*.jpg", full.names = TRUE)
new_files_architecture <- paste0("C:/CIS8392/data/archive/images/images/architecure/architecture.",1:length(architecture_images),".jpg")
file.copy(from = architecture_images, to = new_files_architecture)

art_images <- list.files("C:/CIS8392/data/archive/images/images/art and culture", pattern = "*.jpg", full.names = TRUE)
new_files_art <- paste0("C:/CIS8392/data/archive/images/images/art and culture/art and culture.",1:length(architecture_images),".jpg")
file.copy(from = art_images, to = new_files_art)
})})

```

### Assigning the images to respectve directories

The renamed images are copied to the test, train and validation directories containing the architecture and art and culture sub directories.  
 
The top 1000 images are considered for the training data and the subsequent 500 images are considered for the test and validation dataset.  

```{r Assigned to directories,  message=FALSE, warning=FALSE, results = 'hide'}

invisible({capture.output({
fnames <- paste0("architecture.", 1:1000, ".jpg")
file.copy(file.path(arc_dir, fnames), file.path(train_arc_dir))
fnames <- paste0("architecture.", 1001:1500, ".jpg")
file.copy(file.path(arc_dir, fnames), file.path(validation_arc_dir))
fnames <- paste0("architecture.", 1501:2000, ".jpg")
file.copy(file.path(arc_dir, fnames), file.path(test_arc_dir))
fnames <- paste0("art and culture.", 1:1000, ".jpg")
file.copy(file.path(art_dir, fnames), file.path(train_art_dir))
fnames <- paste0("art and culture.", 1001:1500, ".jpg")
file.copy(file.path(art_dir, fnames), file.path(validation_art_dir))
fnames <- paste0("art and culture.", 1501:2000, ".jpg")
file.copy(file.path(art_dir, fnames), file.path(test_art_dir))
})})

```

### Image count in each directory

```{r Image count, message=FALSE, warning=FALSE}

train_architecture_dir <- "data/images_test/train/architecture/"
cat("total training architecture images:", length(list.files(train_architecture_dir)), "\n")
train_artandculture_dir <- "data/images_test/train/artculture/"
cat("total training art and culture images:", length(list.files(train_artandculture_dir)), "\n")

test_architecture_dir <- "data/images_test/test/architecture/"
cat("total test architecture images:", length(list.files(test_architecture_dir)), "\n")
test_artandculture_dir <- "data/images_test/test/artculture/"
cat("total test art and culture images:", length(list.files(test_artandculture_dir)), "\n")

validation_architecture_dir <- "data/images_test/validation/architecture/"
cat("total validation architecture images:", length(list.files(validation_architecture_dir)), "\n")
validation_artandculture_dir <- "data/images_test/validation/artculture/"
cat("total validation art and culture images:", length(list.files(validation_artandculture_dir)), "\n")
```

## Model Building and Validation

### Data Augmentation

* The image_data_generator function is used to create a datagen variable. We are adding noise atttributes with the imaage_data_generator function such as rotating the image to 40 degree, shifting picture to 20%, zooming the image etc.  
* This results in a set of images for each image that are completely new and looks different from the original image. By this method, we are adding in new images to reduce the overfitting in the data.  

```{r Data Augmentation, message=FALSE, warning=FALSE}

datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40, # randomly rotate images up to 40 degrees
  width_shift_range = 0.2, # randomly shift 20% pictures horizontally
  height_shift_range = 0.2, # randomly shift 20% pictures vertically
  shear_range = 0.2, # randomly apply shearing transformations
  zoom_range = 0.2, # randomly zooming inside pictures
  horizontal_flip = TRUE, # randomly flipping half the images horizontally
  fill_mode = "nearest" # used for filling in newly created pixels
)
fnames <- list.files(train_architecture_dir, full.names = TRUE)
img_path <- fnames[[3]] # Chooses one image to augment
img <- image_load(img_path, target_size = c(150, 150))
img_array <- image_to_array(img) # Converts the shape back to (150, 150, 3)
img_array <- array_reshape(img_array, c(1, 150, 150, 3))
augmentation_generator <- flow_images_from_data(
  img_array,
  generator = datagen,
  batch_size = 1
)
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
```

### Convoluted Neural network with data augentation

* Here, we are building a model with convoluted neural network layers on top of dense layers. We have used three convoluted layers and two dense layers. We have used sigmoid activation function as the final output is binary.  
* We are adding a dropout layer here to reduce the overfitting in the training data.  
* We are building a keras_sequential model with 4 convolution layers and two dense layers.   
* We have specified the loss, optimizer and metrics function which are complied and then added to the model.  
* The summary of the model is displayed.  

```{r Model1, message=FALSE, warning=FALSE}
model_v2 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
model_v2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)
summary(model_v2)
```


In the following code, we have created a test_datagen which doesn't have the augmented images. 
* Next, the train_generator is defined by passing in the augmentation configuration 'datagen'
* Validation_generator is created with the 'test_datagen' without the augmentation configuration.
* The train _generator and validation_generator has the attributes target directories, target_size, batch_size and class_mode. Here we are using "binary" class mode as we need binary labels.
* Then, the model is fitted to the data using the 'fit_generator' function.

```{r Model 1, message=FALSE, warning=FALSE}
test_datagen <- image_data_generator(rescale = 1/255) # no data augmentation
train_generator <- flow_images_from_directory(
  train_dir,
  datagen, 
  target_size = c(150, 150),
  batch_size = 32,
  class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen, 
  target_size = c(150, 150),
  batch_size = 32,
  class_mode = "binary"
)
history_v2 <- model_v2 %>% fit_generator(
  train_generator,
  steps_per_epoch = 50,
  epochs = 15,
  validation_data = validation_generator,
  validation_steps = 50
)
```


### Model Validation

The trained model is now validated against the test images to evaluate the performance of the model. 
```{r Model 1 performance, message=FALSE, warning=FALSE}
test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen, 
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
model_v2 %>% evaluate_generator(test_generator, steps = 50)
```


## Pretrained convnet Model VGG16

Here, we have used the pretrained model "VGG16" 

* First we have initiated the covolution base for the VGG15 model.  
* Next, we have defined a function to extract features from the dataset by running the 'conv_base' on the dataset.   
* The extract_features function is applied on the train,test and validation directory.  


```{r Pretrained convnet model, message=FALSE, warning=FALSE}
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
conv_base


datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20
extract_features <- function(directory, sample_count) {
  features <- array(0, dim = c(sample_count, 4, 4, 512))
  labels <- array(0, dim = c(sample_count))
  generator <- flow_images_from_directory(
    directory = directory, generator = datagen,
    target_size = c(150, 150), batch_size = batch_size, class_mode = "binary"
  )
  i <- 0
  while(TRUE) {
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]; labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)
    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    i <- i + 1
    if (i * batch_size >= sample_count) break
  }
  return(list(features = features,labels = labels))
} 
train <- extract_features(train_dir, 2000) #will take a while since we are running
validation <- extract_features(validation_dir, 1000) # our images through conv_base
test <- extract_features(test_dir, 1000) # still faster than training such a model

```


### Reshape Feature

Next, a new function reshape_feature is created to flatten the last layer of the conv_base. This function is applied on the train,validation and test features extracted earlier.

```{r reshape features, message=FALSE, warning=FALSE}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
```

### Pretrained model fitting

* The model is build with two dense layers and a dropout layer for regularization.   
* The model is trained on the features and labels data from the train dataset created earlier.   
* The training and validation performance is plotte.  
```{r vgg16 model fit, message=FALSE, warning=FALSE}
model_v3 <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu",
              input_shape = 4 * 4 * 512) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")
model_v3 %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
history_v3 <- model_v3 %>% fit(
  train$features, train$labels,
  epochs = 15,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)

plot(history_v3)
```


### Model Evaluation

```{r Model evaluation, message=FALSE, warning=FALSE}
model_v3 %>% evaluate(test$features, test$labels)
```

## Model Summary

* Both the models have performed well considering the fact that only 1000 images were taked from the list of more than 9000 images in the training dataset. 
* It can be seen that the accuracy is higher in the pretrained convnet model (VGG16) compared to the 1st model with augmented data.
* Both the model contained the same number of epochs - '15'
* We can conclude that the VGG16 model is the best fit as it performed really well by classifying almost 96% of records in the test dataset correctly.

## Visualization

Here, we have looked at the images with the most color_hex values. The image with the higher number of color values can be looked upon as the most vibrant or colorful image.  The images with these matching criteria are found in the architecture and art and culture directories.  

* A subset of image data from architecture and art and culture category is considered.  
* The images containing the higher number of color indexes is identified and plotted  

```{r visualization 1,fig.show='hold', message=FALSE, warning=FALSE, fig.height = 20, fig.width = 20, fig.align = "center"}
fnames_arc <- paste0("data/images_test/train/architecture/architecture.", 1:100, ".jpg")
fnames_art <- paste0("data/images_test/train/artculture/art and culture.", 1:100, ".jpg")
col_list <- list("list")
for (i in fnames_arc){
  col_list[i] <- get_colors(i, exclude_col = NULL, exclude_rad = NULL, top_n = NULL,
             min_share = NULL, get_stats = TRUE)
}
col_list_art <- list("list")
for (i in fnames_art){
  col_list_art[i] <- get_colors(i, exclude_col = NULL, exclude_rad = NULL, top_n = NULL,
                            min_share = NULL, get_stats = TRUE)
}
max_color <- 0
c <- vector("list")
for (i in length(col_list)-1) {
  m = length(col_list[[i]])
  if (m > max_color) {
    max_color <- length(col_list[[i]])
    c <- i
  }
}
max_color_art <- 0
e <- vector("list")
for (i in length(col_list_art)-1) {
  m = length(col_list_art[[i]])
  if (m > max_color_art) {
    max_color_art <- length(col_list_art[[i]])
    e <- i
  }
}
colorful_image_arc <- fnames_arc[c]
colorful_image_art <- fnames_art[e]

knitr::include_graphics(c(colorful_image_arc,colorful_image_art))

```

#### From the above visualisation, we can infer that the art and culture category consists of more vibrant colours as found in the images. 


### In the next visualization, a subset of the images color index values is considered for analysis

The length of color index values for a particular image relates with the vibrant and colorful outlook of the image. Hence, if the value is high, more colors are present in that image.   
* Two vectors are created with each vector consisting of 15 images color index values from architecture and art and culture category.   
* We have used the reshape2 library to convert the vector into a data frame.  
* The color index values for subset of images in architecture and art and culture category is plotted using ggplot.   
* The color index value is plotted on the y-axis and the image count on the x-axis (15 images)
* The blue line indicate the art and culture category and the green line indicates the architecture and culture category.  
```{r visualization 2,message=FALSE, warning=FALSE}
l <- vector("list")
for (i in 1:15){
  l[[i]] <- length(col_list_art[[i]])
}
p <- vector("list")
for (i in 1:15){
  p[[i]] <- length(col_list[[i]])
}

require(reshape2)
h <- do.call(cbind, l)
h.melt <- melt(h)
n <- do.call(cbind, p)
n.melt <- melt(n)

require(scales)
point <- format_format(big.mark = " ", decimal.mark = ",", scientific = FALSE)

# Plot it
p  <- ggplot() + geom_line(data = h.melt, aes(x=Var2, y=value),color="blue") + geom_line(data = n.melt, aes(x=Var2, y=value),color = "green")  + 
  geom_point(data = h.melt,aes(x=Var2, y=value)) + geom_point(data = n.melt, aes(x=Var2, y=value)) + theme_light() + xlab("Image count") + ylab("Color Index value") 
  

p + scale_x_continuous(labels = point) + ggtitle(" Total colors in an image") + theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="black", size=14, face="bold"),
axis.title.y = element_text(color="black", size=14, face="bold")) 

```

### From this graph, we can infer the below

* High number of prominent peaks in blue line indicates that the art and culture category of images has more vibrant colors compared to the architecture category.  
* The architecture category seem to be contain very low vibrant images as we can see from the green line. The number of color indexes in the architecture category in very low.  
* We can observe that, there are not much colors associated with architectures and structures. As with most modern day monuments, they are very minimalistic in colors and the same goes with the ancient buildings were the structures resemble uniform colors. Whereas in the art and culture category, we have the people, different cultural celebrations and all these constitues various colors in the images which is refelected in the above graph.



### Image vs Size 

In the below graph, we have visualised the image files in architecture and art and culture with their size
```{r Visualization 3}
id <- 1:10
vis_arc <- list.files("C:/CIS8392/data/images_test/train/architecture/", pattern = "*.jpg", full.names = TRUE)
filesize_arc <- file.size(vis_arc)
filename_arc <- list.files("C:/CIS8392/data/images_test/train/architecture/", pattern = "jpg", recursive = TRUE)
df_arc <- data.frame( id,filename_arc, filesize_arc, stringsAsFactors = F)

vis_art <- list.files("C:/CIS8392/data/images_test/train/artculture/", pattern = "*.jpg", full.names = TRUE)
filesize_art <- file.size(vis_art)

filename_arc <- list.files("C:/CIS8392/data/images_test/train/artculture/", pattern = "jpg", recursive = TRUE)
df_art <- data.frame(id,filename_arc, filesize_art, stringsAsFactors = F)


df_arc <- df_arc[1:10,]
df_art <- df_art[1:10,]
ggplot() + geom_line(data = df_arc, aes(x=id, y=filesize_arc),color="blue") + geom_line(data = df_art, aes(x=id, y=filesize_art),color = "green")  + 
  geom_point(data = df_arc,aes(x=id, y=filesize_arc)) + geom_point(data = df_art, aes(x=id, y=filesize_art)) + theme_light() + xlab("Image") + ylab("Size")  + ggtitle("Image vs Size comparison for Architecture and Art Culture category")

```

* In the above graph, we can see that the file size is high for Art and culture category (green) compared to the Architecture category.   
* It is expected as we have seen in the previous visualization that the art and culture category of images consists of more colors and hence this directly affcts the file size being high in number.